{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T03:22:37.537309Z",
     "start_time": "2025-03-24T03:22:29.040961Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]\n",
      "æ‰§è¡Œç»“æœ: messages=[HumanMessage(content='ä½ å¥½ï¼Œä½ æ˜¯ï¼Ÿ', additional_kwargs={}, response_metadata={})]\n",
      "================\n",
      "client=<openai.resources.chat.completions.completions.Completions object at 0x112a35bb0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x112a54200> root_client=<openai.OpenAI object at 0x107635610> root_async_client=<openai.AsyncOpenAI object at 0x112a35c10> model_name='deepseek-chat' model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.deepseek.com/v1'\n",
      "æ‰§è¡Œç»“æœ: content='æ‚¨å¥½ï¼æˆ‘æ˜¯ç”±ä¸­å›½çš„æ·±åº¦æ±‚ç´¢ï¼ˆDeepSeekï¼‰å…¬å¸å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹DeepSeek-V3ã€‚å¦‚æ‚¨æœ‰ä»»ä½•ä»»ä½•é—®é¢˜ï¼Œæˆ‘ä¼šå°½æˆ‘æ‰€èƒ½ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 7, 'total_tokens': 44, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 7}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_3a5770e1b4_prod0225', 'finish_reason': 'stop', 'logprobs': None} id='run-8c0db09b-1472-4cd2-8e33-8d8bd62e5480-0' usage_metadata={'input_tokens': 7, 'output_tokens': 37, 'total_tokens': 44}\n",
      "================\n",
      "\n",
      "æ‰§è¡Œç»“æœ: æ‚¨å¥½ï¼æˆ‘æ˜¯ç”±ä¸­å›½çš„æ·±åº¦æ±‚ç´¢ï¼ˆDeepSeekï¼‰å…¬å¸å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹DeepSeek-V3ã€‚å¦‚æ‚¨æœ‰ä»»ä½•ä»»ä½•é—®é¢˜ï¼Œæˆ‘ä¼šå°½æˆ‘æ‰€èƒ½ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚\n",
      "================\n",
      "æ‚¨å¥½ï¼æˆ‘æ˜¯ç”±ä¸­å›½çš„æ·±åº¦æ±‚ç´¢ï¼ˆDeepSeekï¼‰å…¬å¸å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹DeepSeek-V3ã€‚å¦‚æ‚¨æœ‰ä»»ä½•ä»»ä½•é—®é¢˜ï¼Œæˆ‘ä¼šå°½æˆ‘æ‰€èƒ½ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from uuid import UUID\n",
    "\n",
    "from langchain_core.outputs import GenerationChunk, ChatGenerationChunk\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import (ChatPromptTemplate, PromptTemplate)\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import Any, Optional, Union\n",
    "\n",
    "# 1.ç¼–æ’Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"{query}\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"deepseek-chat\", api_key=os.environ.get(\"DS_KEY\"), base_url=os.environ.get(\"DS_API_BASE\"))\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "\n",
    "class Chain:\n",
    "    steps: list = []\n",
    "\n",
    "    def __init__(self, steps):\n",
    "        self.steps = steps\n",
    "\n",
    "    def invoke(self, input: Any) -> Any:\n",
    "        output: Any = input\n",
    "        for step in self.steps:\n",
    "            output = step.invoke(output)\n",
    "            print(step)\n",
    "            print(\"æ‰§è¡Œç»“æœ:\", output)\n",
    "            print(\"================\")\n",
    "        return output\n",
    "\n",
    "\n",
    "chain = Chain([prompt, llm, parser])\n",
    "print(chain.invoke({\"query\": \"ä½ å¥½ï¼Œä½ æ˜¯ï¼Ÿ\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc10d8871dac197",
   "metadata": {},
   "source": [
    "## è‡ªå®šä¹‰Chainçš„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566dd441bcb66d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import (ChatPromptTemplate, PromptTemplate)\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import Any\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# 1.ç¼–æ’Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"{query}\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"deepseek-chat\", api_key=os.environ.get(\"DS_KEY\"), base_url=os.environ.get(\"DS_API_BASE\"))\n",
    "\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "#ç­‰ä»·äºä¸€ä¸‹å†™æ³•\n",
    "# composed_chain_with_pip = (\n",
    "#     RunnableParallel({\"query\": RunnablePassthrough()})\n",
    "#     .pipe(prompt)\n",
    "#     .pipe(llm)\n",
    "#     .pipe(parser)\n",
    "# )\n",
    "\n",
    "print(chain.invoke({\"query\": \"ä½ å¥½ï¼Œä½ æ˜¯?\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61086ead172c0a97",
   "metadata": {},
   "source": [
    "## RunnablePassthrough ä¼ é€’æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50a3887ded112aab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T07:33:57.322320Z",
     "start_time": "2025-03-25T07:33:48.821228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼æˆ‘æ˜¯DeepSeek Chatï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸æ‰“é€ çš„æ™ºèƒ½AIåŠ©æ‰‹ã€‚æˆ‘å¯ä»¥å¸®ä½ è§£ç­”é—®é¢˜ã€æä¾›å»ºè®®ã€èŠå¤©äº¤æµï¼Œè¿˜èƒ½å¤„ç†å„ç§æ–‡æœ¬å’Œæ–‡ä»¶å†…å®¹ã€‚å¦‚æœæœ‰ä»»ä½•éœ€è¦ï¼Œå°½ç®¡é—®æˆ‘å§ï¼ğŸ˜Š  \n",
      "\n",
      "æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å‘¢ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import (ChatPromptTemplate, PromptTemplate)\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import Any\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "# 1.ç¼–æ’Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"{query}\")\n",
    "\n",
    "# æ„å»ºå¤§è¯­è¨€æ¨¡å‹\n",
    "llm = ChatOpenAI(model=\"deepseek-chat\", api_key=os.environ.get(\"DS_KEY\"), base_url=os.environ.get(\"DS_API_BASE\"))\n",
    "\n",
    "# åˆ›å»ºé“¾\n",
    "#æ–¹å¼ä¸€\n",
    "# chain = {\"query\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "# \n",
    "# content = chain.invoke(\"ä½ å¥½,ä½ æ˜¯ï¼Ÿ\")\n",
    "# print(content)\n",
    "\n",
    "#æ–¹å¼äºŒ\n",
    "chain = {\"query\": itemgetter(\"query\")} | prompt | llm | StrOutputParser()\n",
    "content = chain.invoke({\"query\": \"ä½ å¥½,ä½ æ˜¯ï¼Ÿ\"})\n",
    "print(content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdb8666a06b0aac",
   "metadata": {},
   "source": [
    "## ä¼ é€’çš„æ•°æ®ä¸­æ·»åŠ æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89417c8471ee98e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T07:42:35.230160Z",
     "start_time": "2025-03-25T07:42:27.217919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰§è¡Œæ£€ç´¢ {'query': 'ä½ å¥½æˆ‘å«ä»€ä¹ˆ'}\n",
      "ä½ å¥½ï¼ŒVVï¼æ ¹æ®æˆ‘ä»¬çš„å¯¹è¯è®°å½•ï¼Œä½ çš„åå­—æ˜¯VVï¼Œæ˜¯ä¸€åAIåº”ç”¨å¼€å‘å·¥ç¨‹å¸ˆã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import (ChatPromptTemplate, PromptTemplate)\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import Any\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def retrieval(query: str) -> str:\n",
    "    \"\"\"æ¨¡æ‹Ÿä¸€ä¸ªæ£€ç´¢å™¨ï¼Œä¼ å…¥queryï¼Œè¾“å‡ºæ–‡æœ¬\"\"\"\n",
    "    print(\"æ‰§è¡Œæ£€ç´¢\", query)\n",
    "    return \"æˆ‘å«VVï¼Œæ˜¯ä¸€åAIåº”ç”¨å¼€å‘å·¥ç¨‹å¸ˆ\"\n",
    "\n",
    "\n",
    "# 1.ç¼–æ’Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"è¯·æ ¹æ®ç”¨æˆ·çš„æé—®å›ä»–é—®é¢˜ï¼Œå¯ä»¥å‚è€ƒä¸Šä¸‹æ–‡è¿›è¡Œå›å¤\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "ç”¨æˆ·çš„é—®é¢˜æ˜¯:{query}\n",
    "\"\"\")\n",
    "\n",
    "# æ„å»ºå¤§è¯­è¨€æ¨¡å‹\n",
    "llm = ChatOpenAI(model=\"deepseek-chat\", api_key=os.environ.get(\"DS_KEY\"), base_url=os.environ.get(\"DS_API_BASE\"))\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = (\n",
    "        RunnablePassthrough.assign(context=lambda query: retrieval(query)) |\n",
    "        prompt |\n",
    "        llm |\n",
    "        parser\n",
    ")\n",
    "content = chain.invoke({\"query\": \"ä½ å¥½æˆ‘å«ä»€ä¹ˆ\"})\n",
    "\n",
    "print(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1d5222f33c057e",
   "metadata": {},
   "source": [
    "## åˆ©ç”¨å›è°ƒåŠŸèƒ½è°ƒè¯•é“¾åº”ç”¨-è®©è¿‡ç¨‹æ›´é€æ˜\n",
    "Callback æ˜¯ LangChain æä¾›çš„å›è°ƒæœºåˆ¶ï¼Œå…è®¸æˆ‘ä»¬åœ¨ LLM åº”ç”¨ç¨‹åºçš„å„ä¸ªé˜¶æ®µä½¿ç”¨ hook (é’© å­)ã€‚é’©å­çš„å«ä¹‰ä¹Ÿéå¸¸ç®€å•ï¼Œæˆ‘ä»¬æŠŠåº”ç”¨ç¨‹åºçœ‹æˆä¸€ä¸ªä¸€ä¸ªçš„å¤„ç†é€»è¾‘ï¼Œä»å¼€å§‹åˆ°ç»“æŸï¼Œé’©å­å°±æ˜¯åœ¨ äº‹ä»¶ä¼ é€åˆ°ç»ˆç‚¹å‰æˆªè·å¹¶ç›‘æ§äº‹ä»¶çš„ä¼ è¾“ã€‚\n",
    "\n",
    "Callback æ”¶é›†åˆ°çš„ä¿¡æ¯å¯ä»¥ç›´æ¥è¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œä¹Ÿå¯ä»¥è¾“å‡ºåˆ°æ–‡ä»¶ï¼Œæ›´å¯ä»¥è¾“å…¥åˆ°ç¬¬ä¸‰æ–¹åº”ç”¨ï¼Œç›¸å½“äº ç‹¬ç«‹çš„æ—¥å¿—ç®¡ç†ç³»ç»Ÿï¼Œé€šè¿‡è¿™äº›æ—¥å¿—å°±å¯ä»¥åˆ†æåº”ç”¨çš„è¿è¡Œæƒ…å†µï¼Œç»Ÿè®¡å¼‚å¸¸ç‡ï¼Œè¿è¡Œçš„ç“¶é¢ˆæ¨¡å—ä»¥ä¾¿ä¼˜åŒ–ã€‚\n",
    "1. CallbackHandler:å¯¹æ¯ä¸ªåº”ç”¨åœºæ™¯æ¯”å¦‚ Agent æˆ– Chain æˆ– Tool çš„çºªå½•ã€‚\n",
    "2. CallbackManager:å¯¹æ‰€æœ‰ CallbackHandler çš„å°è£…å’Œç®¡ç†ï¼ŒåŒ…æ‹¬äº†å•ä¸ªåœºæ™¯çš„ Handleï¼Œä¹ŸåŒ…æ‹¬ è¿è¡Œæ—¶æ•´æ¡é“¾è·¯çš„ Handleã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c641a9911354552",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T08:01:09.781902Z",
     "start_time": "2025-03-25T08:01:01.800874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object RunnableSequence.stream at 0x115d784f0>\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableSequence chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableParallel<query> chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnablePassthrough chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ChatPromptTemplate chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StrOutputParser chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import (ChatPromptTemplate, PromptTemplate)\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import Any\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "\n",
    "# 1.ç¼–æ’Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"{query}\")\n",
    "\n",
    "# æ„å»ºå¤§è¯­è¨€æ¨¡å‹\n",
    "llm = ChatOpenAI(model=\"deepseek-chat\", api_key=os.environ.get(\"DS_KEY\"), base_url=os.environ.get(\"DS_API_BASE\"))\n",
    "\n",
    "chain = {\"query\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "\n",
    "content = chain.stream(\"ä½ å¥½ä½ æ˜¯ï¼Ÿ\", config={\n",
    "    \"callbacks\": [StdOutCallbackHandler()]\n",
    "})\n",
    "print(content)\n",
    "\n",
    "for chunk in content:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f155fe676857b846",
   "metadata": {},
   "source": [
    "## è‡ªå®šä¹‰å›è°ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74ba19c5597cded4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T08:42:56.593346Z",
     "start_time": "2025-03-25T08:42:49.627466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object RunnableSequence.stream at 0x1117b09a0>\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableSequence chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableParallel<query> chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnablePassthrough chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ChatPromptTemplate chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "on_llm_start serialized: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'], 'kwargs': {'model_name': 'deepseek-chat', 'temperature': 0.7, 'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}, 'openai_api_base': 'https://api.deepseek.com/v1', 'max_retries': 2, 'n': 1}, 'name': 'ChatOpenAI'}\n",
      "on_llm_start_ prompts ['Human: ä½ å¥½ä½ æ˜¯ï¼Ÿ']\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StrOutputParser chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.callbacks import  BaseCallbackHandler,StdOutCallbackHandler\n",
    "from uuid import UUID\n",
    "from typing import Dict, Any, List, Optional\n",
    "from typing import Optional,Any,Union\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "class LLMOpsCallbackHandler(BaseCallbackHandler):\n",
    "    \"\"\"è‡ªå®šä¹‰LLMOpså›è°ƒ\"\"\"\n",
    "    def on_llm_start(\n",
    "        self,\n",
    "        serialized: dict[str, Any],\n",
    "        prompts: list[str],\n",
    "        *,\n",
    "        run_id: UUID,\n",
    "        parent_run_id: Optional[UUID] = None,\n",
    "        tags: Optional[list[str]] = None,\n",
    "        metadata: Optional[dict[str, Any]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Any:\n",
    "        print(\"on_llm_start serialized:\",serialized)\n",
    "        print(\"on_llm_start_ prompts\",prompts)\n",
    "\n",
    "    # def on_llm_new_token(\n",
    "    #     self,\n",
    "    #     token: str,\n",
    "    #     *,\n",
    "    #     chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,\n",
    "    #     run_id: UUID,\n",
    "    #     parent_run_id: Optional[UUID] = None,\n",
    "    #     **kwargs: Any,\n",
    "    # ) -> Any:\n",
    "    #    print(\"on_llm_new_token:\", token)\n",
    "       \n",
    "       \n",
    "# 1.ç¼–æ’Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"{query}\")\n",
    "\n",
    "# æ„å»ºå¤§è¯­è¨€æ¨¡å‹\n",
    "llm = ChatOpenAI(model=\"deepseek-chat\", api_key=os.environ.get(\"DS_KEY\"), base_url=os.environ.get(\"DS_API_BASE\"))\n",
    "\n",
    "chain = {\"query\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "\n",
    "content = chain.stream(\"ä½ å¥½ä½ æ˜¯ï¼Ÿ\", config={\n",
    "    \"callbacks\": [StdOutCallbackHandler(),LLMOpsCallbackHandler()]\n",
    "})\n",
    "print(content)\n",
    "\n",
    "for chunk in content:\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
